{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 03 - End-to-End CNN+LSTM Training (Run 6a)\nFine-tune ResNet-18 layer4 + LSTM end-to-end on Drive&Act Kinect IR.\n\n**Key change from Run 3:** CNN layer4 unfrozen with differential LR (CNN=1e-5, LSTM=1e-3).\nEverything else matches Run 3 (best MPCA=39.2%).\n\n**Runtime:** GPU required (T4 16GB). Mixed precision enabled."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Colab Setup\nimport os\nIN_COLAB = 'COLAB_GPU' in os.environ or os.path.exists('/content')\n\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    REPO_DIR = '/content/Driver-Activity-Recognition'\n    if not os.path.exists(REPO_DIR):\n        !git clone https://github.com/batuhne/Driver-Activity-Recognition.git {REPO_DIR}\n\n    os.chdir(REPO_DIR)\n    !pip install -q -r requirements.txt\n    DATA_ROOT = '/content/drive/MyDrive/DriveAndAct'\nelse:\n    DATA_ROOT = './data'\n\nprint(f'Working directory: {os.getcwd()}')\nprint(f'Data root: {DATA_ROOT}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom src.utils import load_config, set_seed\nfrom src.train import train\n\nconfig = load_config()\nif IN_COLAB:\n    config['data']['root'] = DATA_ROOT\n    # Save outputs to Drive so they persist between sessions\n    drive_output = os.path.join(DATA_ROOT, 'results')\n    config['output']['checkpoint_dir'] = os.path.join(drive_output, 'checkpoints')\n    config['output']['log_dir'] = os.path.join(drive_output, 'logs')\n    config['output']['figure_dir'] = os.path.join(drive_output, 'figures')\n\n# ==================== Run 6a: End-to-End CNN Fine-Tuning ====================\n# Single change from Run 3: unfreeze ResNet-18 layer4 and train end-to-end.\n# All other hyperparams match Run 3 (best MPCA=39.2%).\n\n# Model config — same as Run 3\nconfig['model']['use_layernorm'] = True\nconfig['model']['bidirectional'] = True\nconfig['model']['pooling'] = 'attention'\nconfig['model']['lstm_hidden'] = 256            # Run 3 value\nconfig['model']['lstm_dropout'] = 0.3           # Run 3 value\nconfig['model']['freeze_mode'] = 'layer4'       # NEW: only layer4 trainable\n\n# Training config — end-to-end mode\nconfig['training']['mode'] = 'end_to_end'       # NEW: end-to-end training\nconfig['training']['batch_size'] = 8            # Reduced for GPU memory\nconfig['training']['accumulation_steps'] = 4    # Effective batch = 32\nconfig['training']['lr'] = 0.001                # LSTM LR (Run 3)\nconfig['training']['cnn_lr'] = 1e-5             # NEW: CNN fine-tuning LR\nconfig['training']['cnn_warmup_epochs'] = 3     # NEW: freeze CNN for 3 epochs\nconfig['training']['use_amp'] = True            # NEW: mixed precision\nconfig['training']['loss_type'] = 'ce'\nconfig['training']['label_smoothing'] = 0.1\nconfig['training']['mixup_alpha'] = 0.0         # Disabled for video frames\nconfig['training']['noise_std'] = 0.0           # Disabled — CNN augmentation sufficient\nconfig['training']['weight_decay'] = 0.0001     # Run 3 value\nconfig['training']['epochs'] = 50\nconfig['training']['early_stop_patience'] = 12\nconfig['training']['scheduler_type'] = 'plateau'\nconfig['training']['scheduler_factor'] = 0.5\nconfig['training']['scheduler_patience'] = 5\nconfig['training']['gradient_clip'] = 1.0\nconfig['training']['use_weighted_sampler'] = True   # Run 3 value\nconfig['training']['en_beta'] = 0.99                # Run 3 value\nconfig['training']['num_workers'] = 2               # Colab compatible\n\nprint(f'GPU available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\nprint(f\"\\n--- Run 6a Config ---\")\nprint(f\"Mode: {config['training']['mode']}\")\nprint(f\"CNN freeze_mode: {config['model']['freeze_mode']}\")\nprint(f\"CNN LR: {config['training']['cnn_lr']}, LSTM LR: {config['training']['lr']}\")\nprint(f\"CNN warmup: {config['training']['cnn_warmup_epochs']} epochs\")\nprint(f\"Batch: {config['training']['batch_size']} x {config['training']['accumulation_steps']} accum = {config['training']['batch_size'] * config['training']['accumulation_steps']} effective\")\nprint(f\"AMP: {config['training']['use_amp']}\")\nprint(f\"LSTM: h={config['model']['lstm_hidden']}, BiLSTM={config['model']['bidirectional']}, pool={config['model']['pooling']}\")\nprint(f\"Sampler: {'WeightedRandom' if config['training']['use_weighted_sampler'] else 'DISABLED'}\")"
  },
  {
   "cell_type": "code",
   "source": "# Run 6a Diagnostics: verify CNN layer freeze status, param counts, memory estimate\nimport torch\nfrom src.models import CNNLSTMModel\n\nfreeze_mode = config['model']['freeze_mode']\nmodel_check = CNNLSTMModel(\n    num_classes=34,  # approximate\n    hidden_dim=config['model']['lstm_hidden'],\n    num_layers=config['model']['lstm_layers'],\n    lstm_dropout=config['model']['lstm_dropout'],\n    fc_dropout=config['model']['fc_dropout'],\n    use_layernorm=config['model'].get('use_layernorm', False),\n    bidirectional=config['model'].get('bidirectional', False),\n    pooling=config['model'].get('pooling', 'last'),\n    freeze_mode=freeze_mode,\n)\n\ntotal_params = sum(p.numel() for p in model_check.parameters())\ntrainable_params = sum(p.numel() for p in model_check.parameters() if p.requires_grad)\nfrozen_params = total_params - trainable_params\n\nprint(f\"=== CNNLSTMModel (freeze_mode={freeze_mode}) ===\")\nprint(f\"Total params:     {total_params:>10,}\")\nprint(f\"Trainable params: {trainable_params:>10,}\")\nprint(f\"Frozen params:    {frozen_params:>10,}\")\nprint()\n\n# CNN layer breakdown\nprint(\"CNN Layer Status:\")\nfor idx, (name, child) in enumerate(model_check.cnn.features.named_children()):\n    child_params = sum(p.numel() for p in child.parameters())\n    child_trainable = sum(p.numel() for p in child.parameters() if p.requires_grad)\n    if child_params > 0:\n        status = \"TRAINABLE\" if child_trainable > 0 else \"frozen\"\n        print(f\"  [{idx}] {name:>2}: {child_params:>10,} params  [{status}]\")\n\n# LSTM params\nlstm_params = sum(p.numel() for p in model_check.lstm.parameters())\nprint(f\"\\nLSTM params: {lstm_params:,}\")\n\n# Memory estimate\nbatch = config['training']['batch_size']\nseq = 16\nprint(f\"\\nMemory estimate (batch={batch}, seq={seq}, AMP={config['training']['use_amp']}):\")\ninput_mb = batch * seq * 3 * 224 * 224 * 4 / 1024**2\nprint(f\"  Input tensor: ~{input_mb:.0f} MB\")\nprint(f\"  Estimated peak: ~4-6 GB (T4 has 16 GB)\")\n\ndel model_check",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# GPU memory summary (run after training)\nif torch.cuda.is_available():\n    peak_mem = torch.cuda.max_memory_allocated() / 1024**3\n    current_mem = torch.cuda.memory_allocated() / 1024**3\n    total_mem = torch.cuda.get_device_properties(0).total_mem / 1024**3\n    print(f\"GPU Memory: peak={peak_mem:.2f} GB, current={current_mem:.2f} GB, total={total_mem:.1f} GB\")\n    print(f\"Utilization: {peak_mem/total_mem*100:.0f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training\n",
    "Launch TensorBoard to monitor training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TensorBoard (works in Colab and Jupyter)\n%load_ext tensorboard\n%tensorboard --logdir {config['output']['log_dir']}"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}