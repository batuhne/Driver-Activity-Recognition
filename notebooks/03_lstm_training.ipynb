{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - LSTM Training\n",
    "Train ActivityLSTM on pre-extracted ResNet-18 features.\n",
    "\n",
    "**Runtime:** GPU recommended for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Colab Setup\nimport os\nIN_COLAB = 'COLAB_GPU' in os.environ or os.path.exists('/content')\n\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    REPO_DIR = '/content/Driver-Activity-Recognition'\n    if not os.path.exists(REPO_DIR):\n        !git clone https://github.com/batuhne/Driver-Activity-Recognition.git {REPO_DIR}\n\n    os.chdir(REPO_DIR)\n    !pip install -q -r requirements.txt\n    DATA_ROOT = '/content/drive/MyDrive/DriveAndAct'\nelse:\n    DATA_ROOT = './data'\n\nprint(f'Working directory: {os.getcwd()}')\nprint(f'Data root: {DATA_ROOT}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom src.utils import load_config, set_seed\nfrom src.train import train\n\nconfig = load_config()\nif IN_COLAB:\n    config['data']['root'] = DATA_ROOT\n    # Save outputs to Drive so they persist between sessions\n    drive_output = os.path.join(DATA_ROOT, 'results')\n    config['output']['checkpoint_dir'] = os.path.join(drive_output, 'checkpoints')\n    config['output']['log_dir'] = os.path.join(drive_output, 'logs')\n    config['output']['figure_dir'] = os.path.join(drive_output, 'figures')\n\n# Model config — Run 5: capacity balance (192 between 128 and 256)\nconfig['model']['use_layernorm'] = True\nconfig['model']['bidirectional'] = True\nconfig['model']['pooling'] = 'attention'\nconfig['model']['lstm_hidden'] = 192            # Up from 128 (Run 4), down from 256 (Run 3) → ~2.0M params\nconfig['model']['lstm_dropout'] = 0.4           # Between 0.3 (Run 4) and 0.5 (Run 3)\n\n# Training config — Run 5: class-weighted loss instead of sampler\nconfig['training']['loss_type'] = 'ce'\nconfig['training']['mixup_alpha'] = 0.2\nconfig['training']['noise_std'] = 0.1\nconfig['training']['weight_decay'] = 0.0005\nconfig['training']['epochs'] = 80\nconfig['training']['early_stop_patience'] = 15\nconfig['training']['lr'] = 0.0005\nconfig['training']['scheduler_type'] = 'plateau'     # Back to plateau (cosine restart hurt in Run 4)\nconfig['training']['scheduler_factor'] = 0.5\nconfig['training']['scheduler_patience'] = 5\nconfig['training']['use_weighted_sampler'] = False    # KEY CHANGE: disable sampler\nconfig['training']['en_beta'] = 0.999                 # Higher beta OK for loss weights (vs 0.99 for sampler)\n\nprint(f'GPU available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\nprint(f\"Model: BiLSTM={config['model']['bidirectional']}, Pooling={config['model']['pooling']}\")\nprint(f\"LSTM hidden={config['model']['lstm_hidden']}, dropout={config['model']['lstm_dropout']}\")\nprint(f\"Loss: {config['training']['loss_type']}, Mixup alpha={config['training']['mixup_alpha']}\")\nprint(f\"Scheduler: {config['training']['scheduler_type']}, LR={config['training']['lr']}\")\nprint(f\"Sampler: {'WeightedRandom' if config['training']['use_weighted_sampler'] else 'DISABLED (shuffle=True)'}\")\nprint(f\"EN beta={config['training']['en_beta']} ({'sampler weights' if config['training']['use_weighted_sampler'] else 'loss weights'})\")"
  },
  {
   "cell_type": "code",
   "source": "# Run 5b Diagnostics: verify sqrt-dampened loss weights and model size\nimport csv\nimport torch\nimport numpy as np\nfrom src.utils import compute_effective_number_weights\nfrom src.models import ActivityLSTM\n\n# Read training labels from manifest\nfeature_dir = os.path.join(config['data']['root'], config['features']['save_dir'])\nmanifest_path = os.path.join(feature_dir, 'train', 'manifest.csv')\nlabels = []\nwith open(manifest_path) as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        labels.append(int(row['label']))\n\nnum_classes = max(labels) + 1\nprint(f\"Training samples: {len(labels)}, Unique classes: {len(set(labels))}, num_classes: {num_classes}\")\n\n# Loss weights: EN beta=0.999 + sqrt dampening\nbeta = config['training']['en_beta']\nraw_w = compute_effective_number_weights(labels, num_classes, beta=beta)\nprint(f\"\\nRaw EN weights (beta={beta}): min={raw_w.min():.4f}, max={raw_w.max():.4f}, ratio={raw_w.max()/raw_w.min():.1f}x\")\n\ndampened_w = torch.sqrt(raw_w)\ndampened_w = dampened_w / dampened_w.mean()\nprint(f\"Sqrt dampened:               min={dampened_w.min():.4f}, max={dampened_w.max():.4f}, ratio={dampened_w.max()/dampened_w.min():.1f}x\")\nprint(f\"Sampler: DISABLED (shuffle=True)\")\n\n# Model param count (target ~2.0M)\nmodel_check = ActivityLSTM(\n    input_dim=config['model']['feature_dim'],\n    hidden_dim=config['model']['lstm_hidden'],\n    num_layers=config['model']['lstm_layers'],\n    num_classes=num_classes,\n    lstm_dropout=config['model']['lstm_dropout'],\n    fc_dropout=config['model']['fc_dropout'],\n    use_layernorm=config['model'].get('use_layernorm', False),\n    bidirectional=config['model'].get('bidirectional', False),\n    pooling=config['model'].get('pooling', 'last'),\n)\ntotal_params = sum(p.numel() for p in model_check.parameters())\nprint(f\"\\nModel params: {total_params:,} (target ~2.0M)\")\nprint(f\"Params/sample ratio: {total_params/len(labels):.0f} (target ~305)\")\ndel model_check",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training\n",
    "Launch TensorBoard to monitor training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TensorBoard (works in Colab and Jupyter)\n%load_ext tensorboard\n%tensorboard --logdir {config['output']['log_dir']}"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}