{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - LSTM Training\n",
    "Train ActivityLSTM on pre-extracted ResNet-18 features.\n",
    "\n",
    "**Runtime:** GPU recommended for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Colab Setup\nimport os\nIN_COLAB = 'COLAB_GPU' in os.environ or os.path.exists('/content')\n\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    REPO_DIR = '/content/Driver-Activity-Recognition'\n    if not os.path.exists(REPO_DIR):\n        !git clone https://github.com/batuhne/Driver-Activity-Recognition.git {REPO_DIR}\n\n    os.chdir(REPO_DIR)\n    !pip install -q -r requirements.txt\n    DATA_ROOT = '/content/drive/MyDrive/DriveAndAct'\nelse:\n    DATA_ROOT = './data'\n\nprint(f'Working directory: {os.getcwd()}')\nprint(f'Data root: {DATA_ROOT}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom src.utils import load_config, set_seed\nfrom src.train import train\n\nconfig = load_config()\nif IN_COLAB:\n    config['data']['root'] = DATA_ROOT\n    # Save outputs to Drive so they persist between sessions\n    drive_output = os.path.join(DATA_ROOT, 'results')\n    config['output']['checkpoint_dir'] = os.path.join(drive_output, 'checkpoints')\n    config['output']['log_dir'] = os.path.join(drive_output, 'logs')\n    config['output']['figure_dir'] = os.path.join(drive_output, 'figures')\n\n# Model config — Run 4: reduced capacity to fight overfitting\nconfig['model']['use_layernorm'] = True\nconfig['model']['bidirectional'] = True\nconfig['model']['pooling'] = 'attention'       # \"last\" | \"mean\" | \"attention\"\nconfig['model']['lstm_hidden'] = 128            # Reduced from 256 (~850K params vs 3.1M)\nconfig['model']['lstm_dropout'] = 0.3           # Reduced from 0.5 (smaller model)\n\n# Training config — Run 4: cosine scheduler + effective number sampling\nconfig['training']['loss_type'] = 'ce'\nconfig['training']['mixup_alpha'] = 0.2\nconfig['training']['noise_std'] = 0.1\nconfig['training']['weight_decay'] = 0.0005\nconfig['training']['epochs'] = 80\nconfig['training']['early_stop_patience'] = 15\nconfig['training']['lr'] = 0.0005               # Reduced from 0.001\nconfig['training']['scheduler_type'] = 'cosine_warm'\nconfig['training']['cosine_T0'] = 10\nconfig['training']['cosine_T_mult'] = 2\nconfig['training']['cosine_eta_min'] = 1e-6\nconfig['training']['en_beta'] = 0.99            # Effective Number sampling\n\nprint(f'GPU available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\nprint(f\"Model: BiLSTM={config['model']['bidirectional']}, Pooling={config['model']['pooling']}\")\nprint(f\"LSTM hidden={config['model']['lstm_hidden']}, dropout={config['model']['lstm_dropout']}\")\nprint(f\"Loss: {config['training']['loss_type']}, Mixup alpha={config['training']['mixup_alpha']}\")\nprint(f\"Scheduler: {config['training']['scheduler_type']}, LR={config['training']['lr']}\")\nprint(f\"EN sampling beta={config['training']['en_beta']}\")"
  },
  {
   "cell_type": "code",
   "source": "# Run 4 Diagnostics: verify EN weights and model size before training\nimport csv\nimport numpy as np\nfrom src.utils import compute_effective_number_weights\nfrom src.models import ActivityLSTM\n\n# Read training labels from manifest\nfeature_dir = os.path.join(config['data']['root'], config['features']['save_dir'])\nmanifest_path = os.path.join(feature_dir, 'train', 'manifest.csv')\nlabels = []\nwith open(manifest_path) as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        labels.append(int(row['label']))\n\nnum_classes = len(set(labels))\nprint(f\"Training samples: {len(labels)}, Classes: {num_classes}\")\n\n# Compare old vs new weights\nfrom src.utils import compute_class_weights_from_labels\nold_w = compute_class_weights_from_labels(labels, num_classes)\nnew_w = compute_effective_number_weights(labels, num_classes, beta=config['training']['en_beta'])\n\nprint(f\"\\nOld (sklearn balanced) — min: {old_w.min():.4f}, max: {old_w.max():.4f}, ratio: {old_w.max()/old_w.min():.1f}x\")\nprint(f\"New (EN beta={config['training']['en_beta']}) — min: {new_w.min():.4f}, max: {new_w.max():.4f}, ratio: {new_w.max()/new_w.min():.1f}x\")\n\n# Model param count\nmodel_check = ActivityLSTM(\n    input_dim=config['model']['feature_dim'],\n    hidden_dim=config['model']['lstm_hidden'],\n    num_layers=config['model']['lstm_layers'],\n    num_classes=num_classes,\n    lstm_dropout=config['model']['lstm_dropout'],\n    fc_dropout=config['model']['fc_dropout'],\n    use_layernorm=config['model'].get('use_layernorm', False),\n    bidirectional=config['model'].get('bidirectional', False),\n    pooling=config['model'].get('pooling', 'last'),\n)\ntotal_params = sum(p.numel() for p in model_check.parameters())\nprint(f\"\\nModel params: {total_params:,} (target ~850K)\")\nprint(f\"Params/sample ratio: {total_params/len(labels):.0f} (target <150)\")\ndel model_check",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training\n",
    "Launch TensorBoard to monitor training progress in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard (works in Colab and Jupyter)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}