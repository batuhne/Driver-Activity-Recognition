{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Exploration\n",
    "Drive&Act Dataset - Kinect IR View\n",
    "\n",
    "**Runtime:** CPU is sufficient for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Setup\n",
    "import os\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or os.path.exists('/content')\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content\n",
    "    !git clone https://github.com/batuhne/Driver-Activity-Recognition.git 2>/dev/null || true\n",
    "    %cd Driver-Activity-Recognition\n",
    "    !pip install -q -r requirements.txt\n",
    "    DATA_ROOT = '/content/drive/MyDrive/DriveAndAct'\n",
    "else:\n",
    "    DATA_ROOT = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.' if not IN_COLAB else '/content/Driver-Activity-Recognition')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from src.utils import load_config, get_activity_labels, build_file_id_to_video_path\n",
    "from src.dataset import parse_annotations\n",
    "\n",
    "config = load_config()\n",
    "if IN_COLAB:\n",
    "    config['data']['root'] = DATA_ROOT\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Annotation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main annotation file\n",
    "ann_path = os.path.join(config['data']['root'], config['data']['annotation_dir'], config['data']['annotation_file'])\n",
    "df = pd.read_csv(ann_path)\n",
    "print(f'Total segments: {len(df)}')\n",
    "print(f'Unique activities: {df[\"activity\"].nunique()}')\n",
    "print(f'Participants: {sorted(df[\"participant_id\"].unique())}')\n",
    "print(f'\\nColumns: {list(df.columns)}')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = df['activity'].value_counts()\n",
    "print(f'Most common: {class_counts.index[0]} ({class_counts.iloc[0]})')\n",
    "print(f'Least common: {class_counts.index[-1]} ({class_counts.iloc[-1]})')\n",
    "print(f'Imbalance ratio: {class_counts.iloc[0] / class_counts.iloc[-1]:.1f}x')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "class_counts.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Number of Segments')\n",
    "ax.set_title('Activity Class Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Segment Duration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment durations\n",
    "df['duration_frames'] = df['frame_end'] - df['frame_start']\n",
    "df['duration_sec'] = df['duration_frames'] / 30.0  # 30 fps\n",
    "\n",
    "print(f'Duration stats (seconds):')\n",
    "print(df['duration_sec'].describe())\n",
    "print(f'\\nSegments < 8 frames: {(df[\"duration_frames\"] < 8).sum()}')\n",
    "print(f'Segments < 16 frames: {(df[\"duration_frames\"] < 16).sum()}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].hist(df['duration_sec'], bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0].set_xlabel('Duration (seconds)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Segment Duration Distribution')\n",
    "axes[0].axvline(x=3.0, color='red', linestyle='--', label='3s (16 frames @ 5fps)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Per-class mean duration\n",
    "mean_dur = df.groupby('activity')['duration_sec'].mean().sort_values()\n",
    "mean_dur.plot(kind='barh', ax=axes[1], color='coral')\n",
    "axes[1].set_xlabel('Mean Duration (seconds)')\n",
    "axes[1].set_title('Mean Segment Duration per Activity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse splits\n",
    "splits, label_to_idx, idx_to_label = parse_annotations(config)\n",
    "print(f'Number of classes: {len(label_to_idx)}')\n",
    "for name, segs in splits.items():\n",
    "    labels = [s[\"label_idx\"] for s in segs]\n",
    "    participants = set(s[\"participant_id\"] for s in segs)\n",
    "    print(f'{name}: {len(segs)} segments, participants: {sorted(participants)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Frame Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample frames from different activities\n",
    "file_id_to_video = build_file_id_to_video_path(config['data']['root'], config['data']['video_dir'])\n",
    "print(f'Found {len(file_id_to_video)} video files')\n",
    "\n",
    "# Pick 6 random segments from different activities\n",
    "sample_activities = np.random.choice(list(label_to_idx.keys()), size=min(6, len(label_to_idx)), replace=False)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for ax, activity in zip(axes.flat, sample_activities):\n",
    "    # Find a segment for this activity\n",
    "    seg = next((s for s in splits['train'] if s['activity'] == activity), None)\n",
    "    if seg is None:\n",
    "        continue\n",
    "    video_path = file_id_to_video.get(seg['file_id'])\n",
    "    if video_path is None:\n",
    "        continue\n",
    "    \n",
    "    mid_frame = (seg['frame_start'] + seg['frame_end']) // 2\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        ax.imshow(frame, cmap='gray' if len(frame.shape) == 2 else None)\n",
    "    ax.set_title(activity.replace('_', ' '), fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Frames from Different Activities', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
